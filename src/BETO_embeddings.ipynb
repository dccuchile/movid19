{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertModel, BertForMaskedLM, BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile_name = '../data/textos_20200518.csv'\n",
    "text_dict = {}\n",
    "\n",
    "id_field = 'id_movid'\n",
    "date_field = 'fecha'\n",
    "text_field = 's3_consulta_pqno_8_TEXT'\n",
    "should_ignore = []\n",
    "\n",
    "with open(infile_name) as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        k = int(row[id_field])\n",
    "        d = row[date_field]\n",
    "        text = row[text_field]\n",
    "        if text not in should_ignore:\n",
    "            text_dict[(k,d)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = '../out/beto'\n",
    "today = '20200518'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "model = BertModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = set('aáeéoóíúiuüàèìòùbcdfghjklmnñopqrstvwxyz')\n",
    "numbers = set('1234567890')\n",
    "ignore_tokens = ['soy']\n",
    "\n",
    "def clean_text(text):\n",
    "    char_tokens = []\n",
    "    text = text.lower().strip()\n",
    "    for char in text:\n",
    "        if char in (letters | numbers):\n",
    "            to_append = char\n",
    "        else:\n",
    "            to_append = ' '\n",
    "        char_tokens.append(to_append)\n",
    "    text = re.sub(' +',' ',''.join(char_tokens)).strip()\n",
    "    return text\n",
    "\n",
    "def delete_ignored_tokens(tokens):\n",
    "    new_tokens = [token for token in tokens if token not in ignore_tokens]\n",
    "    return new_tokens\n",
    "\n",
    "def to_vector(text, tokenizer, model, verbose=True):\n",
    "    text = clean_text(text)\n",
    "    text = '[CLS] ' + text + ' [SEP]'\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens]) # Batch size 1\n",
    "    outputs = model(tokens_tensor)\n",
    "    hidden_size = outputs[0].size()[-1]\n",
    "    vectors = outputs[0].view(-1,768)\n",
    "    vec = torch.mean(vectors, dim=0)\n",
    "    return vec / torch.norm(vec)\n",
    "\n",
    "def out_emb_line(emb):\n",
    "    line = [str(n) for n in emb]\n",
    "    line = '\\t'.join(line) + '\\n'\n",
    "    return line\n",
    "\n",
    "def out_emb_line_torch(emb):\n",
    "    line = [str(n.item()) for n in emb]\n",
    "    line = '\\t'.join(line) + '\\n'\n",
    "    return line\n",
    "\n",
    "def prepare_text_for_bert(text):\n",
    "    text = clean_text(text)\n",
    "    text = '[CLS] ' + text + ' [SEP]'\n",
    "    return text\n",
    "\n",
    "def to_vector_batch(list_of_texts, tokenizer, model, batch_size=None, device='cpu', verbose=True):\n",
    "    if device:\n",
    "      model = model.to(device)\n",
    "    N = len(list_of_texts)\n",
    "    if not batch_size:\n",
    "        batch_size = N\n",
    "    b, i = 0, 0\n",
    "    embs_list = []\n",
    "    while i < N:\n",
    "        j = i + batch_size\n",
    "        if j > N:\n",
    "            j = N\n",
    "        if verbose:\n",
    "            info = f'batch:{b}, examples:{j}/{N}'\n",
    "            print(info)\n",
    "            b += 1\n",
    "        current_list = list_of_texts[i:j]\n",
    "        token_ids = tokenizer.batch_encode_plus(current_list, pad_to_max_length=True)['input_ids']\n",
    "        tensor_token_ids = [torch.tensor(x) for x in token_ids]\n",
    "        model_input = pad_sequence(tensor_token_ids, batch_first=True, padding_value=1)\n",
    "        if device:\n",
    "            model_input = model_input.to(device)\n",
    "        output_embs = model(model_input)[0].data\n",
    "        embeddings = output_embs.mean(dim=1)\n",
    "        embeddings = embeddings.to('cpu')\n",
    "        embs_list.append(embeddings)\n",
    "        del output_embs\n",
    "        del model_input\n",
    "        i = j # next batch\n",
    "    embs = torch.cat(embs_list)\n",
    "    if verbose:\n",
    "        print('done')\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_keys_all = list(text_dict.keys())\n",
    "l_text_all = [prepare_text_for_bert(text_dict[k]) for k in l_keys_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_text = l_text_all\n",
    "l_keys = l_keys_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:0, examples:50/10157\n",
      "batch:1, examples:100/10157\n",
      "batch:2, examples:150/10157\n",
      "batch:3, examples:200/10157\n",
      "batch:4, examples:250/10157\n",
      "batch:5, examples:300/10157\n",
      "batch:6, examples:350/10157\n",
      "batch:7, examples:400/10157\n",
      "batch:8, examples:450/10157\n",
      "batch:9, examples:500/10157\n",
      "batch:10, examples:550/10157\n",
      "batch:11, examples:600/10157\n",
      "batch:12, examples:650/10157\n",
      "batch:13, examples:700/10157\n",
      "batch:14, examples:750/10157\n",
      "batch:15, examples:800/10157\n",
      "batch:16, examples:850/10157\n",
      "batch:17, examples:900/10157\n",
      "batch:18, examples:950/10157\n",
      "batch:19, examples:1000/10157\n",
      "batch:20, examples:1050/10157\n",
      "batch:21, examples:1100/10157\n",
      "batch:22, examples:1150/10157\n",
      "batch:23, examples:1200/10157\n",
      "batch:24, examples:1250/10157\n",
      "batch:25, examples:1300/10157\n",
      "batch:26, examples:1350/10157\n",
      "batch:27, examples:1400/10157\n",
      "batch:28, examples:1450/10157\n",
      "batch:29, examples:1500/10157\n",
      "batch:30, examples:1550/10157\n",
      "batch:31, examples:1600/10157\n",
      "batch:32, examples:1650/10157\n",
      "batch:33, examples:1700/10157\n",
      "batch:34, examples:1750/10157\n",
      "batch:35, examples:1800/10157\n",
      "batch:36, examples:1850/10157\n",
      "batch:37, examples:1900/10157\n",
      "batch:38, examples:1950/10157\n",
      "batch:39, examples:2000/10157\n",
      "batch:40, examples:2050/10157\n",
      "batch:41, examples:2100/10157\n",
      "batch:42, examples:2150/10157\n",
      "batch:43, examples:2200/10157\n",
      "batch:44, examples:2250/10157\n",
      "batch:45, examples:2300/10157\n",
      "batch:46, examples:2350/10157\n",
      "batch:47, examples:2400/10157\n",
      "batch:48, examples:2450/10157\n",
      "batch:49, examples:2500/10157\n",
      "batch:50, examples:2550/10157\n",
      "batch:51, examples:2600/10157\n",
      "batch:52, examples:2650/10157\n",
      "batch:53, examples:2700/10157\n",
      "batch:54, examples:2750/10157\n",
      "batch:55, examples:2800/10157\n",
      "batch:56, examples:2850/10157\n",
      "batch:57, examples:2900/10157\n",
      "batch:58, examples:2950/10157\n",
      "batch:59, examples:3000/10157\n",
      "batch:60, examples:3050/10157\n",
      "batch:61, examples:3100/10157\n",
      "batch:62, examples:3150/10157\n",
      "batch:63, examples:3200/10157\n",
      "batch:64, examples:3250/10157\n",
      "batch:65, examples:3300/10157\n",
      "batch:66, examples:3350/10157\n",
      "batch:67, examples:3400/10157\n",
      "batch:68, examples:3450/10157\n",
      "batch:69, examples:3500/10157\n",
      "batch:70, examples:3550/10157\n",
      "batch:71, examples:3600/10157\n",
      "batch:72, examples:3650/10157\n",
      "batch:73, examples:3700/10157\n",
      "batch:74, examples:3750/10157\n",
      "batch:75, examples:3800/10157\n",
      "batch:76, examples:3850/10157\n",
      "batch:77, examples:3900/10157\n",
      "batch:78, examples:3950/10157\n",
      "batch:79, examples:4000/10157\n",
      "batch:80, examples:4050/10157\n",
      "batch:81, examples:4100/10157\n",
      "batch:82, examples:4150/10157\n",
      "batch:83, examples:4200/10157\n",
      "batch:84, examples:4250/10157\n",
      "batch:85, examples:4300/10157\n",
      "batch:86, examples:4350/10157\n",
      "batch:87, examples:4400/10157\n",
      "batch:88, examples:4450/10157\n",
      "batch:89, examples:4500/10157\n",
      "batch:90, examples:4550/10157\n",
      "batch:91, examples:4600/10157\n",
      "batch:92, examples:4650/10157\n",
      "batch:93, examples:4700/10157\n",
      "batch:94, examples:4750/10157\n",
      "batch:95, examples:4800/10157\n",
      "batch:96, examples:4850/10157\n",
      "batch:97, examples:4900/10157\n",
      "batch:98, examples:4950/10157\n",
      "batch:99, examples:5000/10157\n",
      "batch:100, examples:5050/10157\n",
      "batch:101, examples:5100/10157\n",
      "batch:102, examples:5150/10157\n",
      "batch:103, examples:5200/10157\n",
      "batch:104, examples:5250/10157\n",
      "batch:105, examples:5300/10157\n",
      "batch:106, examples:5350/10157\n",
      "batch:107, examples:5400/10157\n",
      "batch:108, examples:5450/10157\n",
      "batch:109, examples:5500/10157\n",
      "batch:110, examples:5550/10157\n",
      "batch:111, examples:5600/10157\n",
      "batch:112, examples:5650/10157\n",
      "batch:113, examples:5700/10157\n",
      "batch:114, examples:5750/10157\n",
      "batch:115, examples:5800/10157\n",
      "batch:116, examples:5850/10157\n",
      "batch:117, examples:5900/10157\n",
      "batch:118, examples:5950/10157\n",
      "batch:119, examples:6000/10157\n",
      "batch:120, examples:6050/10157\n",
      "batch:121, examples:6100/10157\n",
      "batch:122, examples:6150/10157\n",
      "batch:123, examples:6200/10157\n",
      "batch:124, examples:6250/10157\n",
      "batch:125, examples:6300/10157\n",
      "batch:126, examples:6350/10157\n",
      "batch:127, examples:6400/10157\n",
      "batch:128, examples:6450/10157\n",
      "batch:129, examples:6500/10157\n",
      "batch:130, examples:6550/10157\n",
      "batch:131, examples:6600/10157\n",
      "batch:132, examples:6650/10157\n",
      "batch:133, examples:6700/10157\n",
      "batch:134, examples:6750/10157\n",
      "batch:135, examples:6800/10157\n",
      "batch:136, examples:6850/10157\n",
      "batch:137, examples:6900/10157\n",
      "batch:138, examples:6950/10157\n",
      "batch:139, examples:7000/10157\n",
      "batch:140, examples:7050/10157\n",
      "batch:141, examples:7100/10157\n",
      "batch:142, examples:7150/10157\n",
      "batch:143, examples:7200/10157\n",
      "batch:144, examples:7250/10157\n",
      "batch:145, examples:7300/10157\n",
      "batch:146, examples:7350/10157\n",
      "batch:147, examples:7400/10157\n",
      "batch:148, examples:7450/10157\n",
      "batch:149, examples:7500/10157\n",
      "batch:150, examples:7550/10157\n",
      "batch:151, examples:7600/10157\n",
      "batch:152, examples:7650/10157\n",
      "batch:153, examples:7700/10157\n",
      "batch:154, examples:7750/10157\n",
      "batch:155, examples:7800/10157\n",
      "batch:156, examples:7850/10157\n",
      "batch:157, examples:7900/10157\n",
      "batch:158, examples:7950/10157\n",
      "batch:159, examples:8000/10157\n",
      "batch:160, examples:8050/10157\n",
      "batch:161, examples:8100/10157\n",
      "batch:162, examples:8150/10157\n",
      "batch:163, examples:8200/10157\n",
      "batch:164, examples:8250/10157\n",
      "batch:165, examples:8300/10157\n",
      "batch:166, examples:8350/10157\n",
      "batch:167, examples:8400/10157\n",
      "batch:168, examples:8450/10157\n",
      "batch:169, examples:8500/10157\n",
      "batch:170, examples:8550/10157\n",
      "batch:171, examples:8600/10157\n",
      "batch:172, examples:8650/10157\n",
      "batch:173, examples:8700/10157\n",
      "batch:174, examples:8750/10157\n",
      "batch:175, examples:8800/10157\n",
      "batch:176, examples:8850/10157\n",
      "batch:177, examples:8900/10157\n",
      "batch:178, examples:8950/10157\n",
      "batch:179, examples:9000/10157\n",
      "batch:180, examples:9050/10157\n",
      "batch:181, examples:9100/10157\n",
      "batch:182, examples:9150/10157\n",
      "batch:183, examples:9200/10157\n",
      "batch:184, examples:9250/10157\n",
      "batch:185, examples:9300/10157\n",
      "batch:186, examples:9350/10157\n",
      "batch:187, examples:9400/10157\n",
      "batch:188, examples:9450/10157\n",
      "batch:189, examples:9500/10157\n",
      "batch:190, examples:9550/10157\n",
      "batch:191, examples:9600/10157\n",
      "batch:192, examples:9650/10157\n",
      "batch:193, examples:9700/10157\n",
      "batch:194, examples:9750/10157\n",
      "batch:195, examples:9800/10157\n",
      "batch:196, examples:9850/10157\n",
      "batch:197, examples:9900/10157\n",
      "batch:198, examples:9950/10157\n",
      "batch:199, examples:10000/10157\n",
      "batch:200, examples:10050/10157\n",
      "batch:201, examples:10100/10157\n",
      "batch:202, examples:10150/10157\n",
      "batch:203, examples:10157/10157\n",
      "done\n",
      "CPU times: user 29min 53s, sys: 3min 8s, total: 33min 2s\n",
      "Wall time: 19min 41s\n"
     ]
    }
   ],
   "source": [
    "%time embs = to_vector_batch(l_text, tokenizer, model, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dict = {}\n",
    "\n",
    "for emb, id_t in zip(embs,l_keys):\n",
    "    emb_dict[id_t] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfilename_we = output + '_' + today + '_emb.tsv'\n",
    "outfilename_metadata = output + '_' + today + '_metadata.tsv'\n",
    "\n",
    "with open(outfilename_we,'w') as outfile_we, open(outfilename_metadata,'w') as outfile_metadata:\n",
    "    header_line = 'movid_id\\ttexto\\n'\n",
    "    outfile_metadata.write(header_line)\n",
    "    for id_t in emb_dict:\n",
    "        k, date = id_t\n",
    "        text = text_dict[id_t]\n",
    "        text_line = f'{k}\\t{text}\\n'\n",
    "        outfile_metadata.write(text_line)\n",
    "        \n",
    "        emb = emb_dict[id_t]\n",
    "        emb_line = out_emb_line_torch(emb)\n",
    "        outfile_we.write(emb_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora clasifica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters observados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datos base (entrenamiento)\n",
    "from classes_and_labels import train_data, labels, labels_to_classes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computa embeddings para los textos observados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train_data = []\n",
    "for cluster in train_data:\n",
    "    embs = []\n",
    "    for text in cluster:\n",
    "        emb = to_vector(text,  tokenizer, model)\n",
    "        embs.append(emb)\n",
    "    emb_train_data.append(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_text(text_1, text_2, wordvectors):\n",
    "    vec_1 = to_vector(text_1, wordvectors)\n",
    "    vec_2 = to_vector(text_2, wordvectors)\n",
    "    return similarity(vec_1, vec_2)\n",
    "\n",
    "\n",
    "def similarity(vec_1, vec_2):\n",
    "    sim = vec_1 @ vec_2\n",
    "    return sim\n",
    "\n",
    "def similarity_to_cluster(vec_1, cluster, dist=0):\n",
    "    sims = []\n",
    "    for vec in cluster:\n",
    "        sim = similarity(vec_1, vec)\n",
    "        sims.append(sim)\n",
    "    if dist == 0:\n",
    "        return np.max(sims)\n",
    "    elif dist == 1:\n",
    "        return np.average(sims)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def most_similar(vec_1, vec_set):\n",
    "    best_sim = 0\n",
    "    out = -1\n",
    "    for i,vec in enumerate(vec_set):\n",
    "        sim = similarity(vec_1, vec)\n",
    "        if sim > best_sim:\n",
    "            out = i\n",
    "            best_sim = sim\n",
    "    return out, best_sim\n",
    "    \n",
    "def emb_classify(vec1, emb_train_data, dist=0):\n",
    "    best_sim = 0\n",
    "    predicted_class = -1\n",
    "    for c, cluster in enumerate(emb_train_data):\n",
    "        sim = similarity_to_cluster(vec1, cluster, dist)\n",
    "        if sim > best_sim:\n",
    "            predicted_class = c  \n",
    "            best_sim = sim\n",
    "    return predicted_class, best_sim\n",
    "\n",
    "def save_out_classes(text_cls, text_sims, outfilename):\n",
    "    no_class = ['NA','']\n",
    "    \n",
    "    with open(outfilename, 'w') as outfile:\n",
    "        fieldnames = ['id_movid', 'fecha']\n",
    "        fieldnames += ['con_'+str(i) for i in range(1, len(classes))]\n",
    "        fieldnames += ['class_id', 'text','class', 'sim']\n",
    "        \n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for k in text_cls:\n",
    "            row = {}\n",
    "            i, date = k\n",
    "            row['id_movid'] = i\n",
    "            row['fecha'] = date\n",
    "            row['text'] = text_dict[k]\n",
    "            \n",
    "            for j in range(1, len(classes)):\n",
    "                row['con_'+str(j)] = 0\n",
    "            \n",
    "            if row['text'] in no_class:\n",
    "                class_id = 0\n",
    "                row['sim'] = 0\n",
    "            else:\n",
    "                class_id = labels_to_classes[text_cls[k]]\n",
    "                row['sim'] = text_sims[k]\n",
    "                row['con_'+str(class_id)] = 1\n",
    "                \n",
    "            row['class_id'] = class_id\n",
    "            row['class'] = classes[class_id]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un dia de cefalea que cedió ante analgesia / Sintomas leves o transitorios\n",
      "Dolor de garganta autolimitado por dos dias / Sintomas leves o transitorios\n",
      "Jaquecosa / Sintomas leves o transitorios\n",
      "Me he sentido bien / Considera que no tiene riesgo\n",
      "Porque los síntomas no cumplen con los criterios para la acudir al sistema de salud / Sintomas habituales o atribuye a otra causa\n",
      "Por ser Dolor tipo migraña que tengo habitualmente / Sintomas leves o transitorios\n",
      "Lo asimilo ai trabajo / Considera que no tiene riesgo\n",
      "porque padezco de migrañas y estoy con tratamiento / Considera que no tiene riesgo\n",
      "Es un sintoma habitual cada año en esta precisa trempoarada / Sintomas habituales o atribuye a otra causa\n",
      "Considero la prioridad de otras personas en el sistema de salud / Considera que no tiene riesgo\n",
      "Soy médico / Sintomas leves o transitorios\n",
      "Reconozco dolor de cabeza como algo tensional / Sintomas leves o transitorios\n",
      "Asma descompensado por mala adherencia / Considera que no tiene riesgo\n",
      "SUFRO DE ASMA  EPOC Y TOS CRONICA / Sintomas leves o transitorios\n",
      "Porque es parte de diagnóstico de base: Esclerodermia / Considera que no tiene riesgo\n",
      "Soy profesional médico / Sintomas leves o transitorios\n",
      "Quiero asistir lo menos posible a un establecimiento de salud / Considera que no tiene riesgo\n",
      "Porque no hay atención meduca / Considera que no tiene riesgo\n",
      "Prefiero quedarme en casa para no exponerme al virus / Considera que no tiene riesgo\n",
      "normales rn mi y sobre todo en embarazo / Considera que no tiene riesgo\n",
      "porque el dolor de cabeza se debía a que aumenté mi consumo de azúcar y la tos es la cantidad normal que me da en esta época que da más frío (un poco cada 3 días o algo así) / Sintomas habituales o atribuye a otra causa\n",
      "Porque no fue un síntoma que durase más de 1 dia / Considera que no tiene riesgo\n",
      "Soy jaquecosa / Sintomas leves o transitorios\n",
      "sufro de recurrentes dolores de cabeza / Sintomas habituales o atribuye a otra causa\n",
      "Se me quitó / Considera que no tiene riesgo\n",
      "Duro un día la obstruccion / Saturación del sistema de salud\n",
      "Tengo fibromialgia / Sintomas leves o transitorios\n",
      "Vacuna influenza, sintomas / Considera que no tiene riesgo\n",
      "Es común que me duela la garganta / Considera que no tiene riesgo\n",
      "Artrosis / Sintomas leves o transitorios\n",
      "Porque temo que en la consulta este mucha gente o deba esperar demasiado tiempo para ser ate dida / Considera que no tiene riesgo\n",
      "me suele venir crisis de dolor de cabeza cuando estoy muy tensa / Sintomas habituales o atribuye a otra causa\n",
      "Sufro de constantes dolores de cabeza / Sintomas habituales o atribuye a otra causa\n",
      "Porque siempre me duelen los músculos y después se me pasa dolo / Considera que no tiene riesgo\n",
      "Porque soy médico / Considera que no tiene riesgo\n",
      "Estrés / Saturación del sistema de salud\n",
      "Por miedo a contagiarme en sistema de salud y espero a que aparezca mas síntomas / Considera que no tiene riesgo\n",
      "No puedo salir de casa pues vivo sola con 2 niñas pequeñas. No puedo exponerlas a acompañarme y menos a un centro de salud. / Miedo a contagiarse\n",
      "Por que en el consultorio no hay atención de médicos solo entrega de alimentos y medicamentos / Considera que no tiene riesgo\n",
      "Tuve 1 semana encerrado con fiebre, dolor muscular, dolor en espalda alta al aspirar. Cabeza con punzada constante. Sensibilidad a la luz. Lo tomé como gripe pero usando mascarilla por la tos y la sospecha. / Sintomas habituales o atribuye a otra causa\n",
      "Tener que salir a una consulta y exponerme a la infección / Considera que no tiene riesgo\n",
      "Por qué no he presentado ningún síntoma / Considera que no tiene riesgo\n",
      "La perdida de olfato a sido porque estoy congestionada  y según lo que e leído sobre los sintomas del covid 19 la congestión no corresponde como síntoma. / Sintomas habituales o atribuye a otra causa\n",
      "Hoy voy al medico / Saturación del sistema de salud\n",
      "Sd premenstrual / Considera que no tiene riesgo\n",
      "es común que me duela la cabeza / Considera que no tiene riesgo\n",
      "Porque me mejoré tomando aguas de hierbas y fue solo un par de días / Considera que no tiene riesgo\n",
      "Soy asmática y me comunico con mí médico por WhatsApp ya que ir a urgencias es más riesgoso para mí / Sintomas habituales o atribuye a otra causa\n",
      "Por que fue debido a crisis de angustia / Considera que no tiene riesgo\n"
     ]
    }
   ],
   "source": [
    "text_cls = {}\n",
    "text_sims = {}\n",
    "for k in emb_dict:\n",
    "    c, sim = emb_classify(emb_dict[k], emb_train_data, dist=0)\n",
    "    text_cls[k] = c\n",
    "    text_sims[k] = sim\n",
    "    if np.random.rand() > 0.995:\n",
    "        print(text_dict[k], '/', classes[labels_to_classes[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename = f'{output}_{today}_classes.csv'\n",
    "save_out_classes(text_cls, text_sims, outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
