{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necesita embedding vectors en la carpeta ../we/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from numpy.linalg import norm\n",
    "from gensim.models.wrappers import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infile_name = '../data/textos_20200518.csv'\n",
    "text_dict = {}\n",
    "\n",
    "id_field = 'id_movid'\n",
    "date_field = 'fecha'\n",
    "text_field = 's3_consulta_pqno_8_TEXT'\n",
    "should_ignore = []\n",
    "\n",
    "with open(infile_name) as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        k = int(row[id_field])\n",
    "        d = row[date_field]\n",
    "        text = row[text_field]\n",
    "        if text not in should_ignore:\n",
    "            text_dict[(k,d)] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = 'bin' # 'vec\n",
    "wordvectors_file = '../we/fasttext-suc'\n",
    "output = '../out/suc'\n",
    "today = '20200518'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if mode == 'bin':\n",
    "    wordvectors = FastText.load_fasttext_format(wordvectors_file)\n",
    "elif mode == 'vec':\n",
    "    wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file + '.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = set('aáeéoóíúiuüàèìòùbcdfghjklmnñopqrstvwxyz')\n",
    "numbers = set('1234567890')\n",
    "ignore_tokens = ['soy']\n",
    "\n",
    "def clean_text(text):\n",
    "    char_tokens = []\n",
    "    text = text.lower().strip()\n",
    "    for char in text:\n",
    "        if char in (letters | numbers):\n",
    "            to_append = char\n",
    "        else:\n",
    "            to_append = ' '\n",
    "        char_tokens.append(to_append)\n",
    "    text = re.sub(' +',' ',''.join(char_tokens)).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    return text.split()\n",
    "\n",
    "def delete_ignored_tokens(tokens):\n",
    "    new_tokens = [token for token in tokens if token not in ignore_tokens]\n",
    "    return new_tokens\n",
    "\n",
    "def to_vector(text, we, verbose=True):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = delete_ignored_tokens(tokens)\n",
    "    vec = np.zeros(300)\n",
    "    n = 0\n",
    "    for word in tokens:\n",
    "        # si la palabra está la acumulamos\n",
    "        if word in we:\n",
    "            vec += we[word]\n",
    "            n += 1\n",
    "    if norm(vec) == 0 or n == 0:\n",
    "        if verbose:\n",
    "            print('not possible to create vector for:', tokens)\n",
    "        return vec\n",
    "    else:\n",
    "        vec = vec / n\n",
    "        return vec / norm(vec)\n",
    "\n",
    "def out_emb_line(emb):\n",
    "    line = [str(n) for n in emb]\n",
    "    line = '\\t'.join(line) + '\\n'\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not possible to create vector for: []\n",
      "not possible to create vector for: []\n",
      "not possible to create vector for: []\n",
      "not possible to create vector for: []\n",
      "not possible to create vector for: []\n",
      "not possible to create vector for: []\n",
      "not possible to create vector for: []\n"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "\n",
    "for id_t in text_dict:\n",
    "    emb_dict[id_t] = to_vector(text_dict[id_t], wordvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename_we = output + '_' + today + '_emb.tsv'\n",
    "outfilename_metadata = output + '_' + today + '_metadata.tsv'\n",
    "\n",
    "with open(outfilename_we,'w') as outfile_we, open(outfilename_metadata,'w') as outfile_metadata:\n",
    "    header_line = 'movid_id\\ttexto\\n'\n",
    "    outfile_metadata.write(header_line)\n",
    "    for id_t in emb_dict:\n",
    "        k, date = id_t\n",
    "        text = text_dict[id_t]\n",
    "        text_line = f'{k}\\t{text}\\n'\n",
    "        outfile_metadata.write(text_line)\n",
    "        \n",
    "        emb = emb_dict[id_t]\n",
    "        emb_line = out_emb_line(emb)\n",
    "        outfile_we.write(emb_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ahora clasifica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters observados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datos base (entrenamiento)\n",
    "from classes_and_labels import train_data, labels, labels_to_classes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computa embeddings para los textos observados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train_data = []\n",
    "for cluster in train_data:\n",
    "    embs = []\n",
    "    for text in cluster:\n",
    "        emb = to_vector(text,  wordvectors)\n",
    "        embs.append(emb)\n",
    "    emb_train_data.append(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_text(text_1, text_2, wordvectors):\n",
    "    vec_1 = to_vector(text_1, wordvectors)\n",
    "    vec_2 = to_vector(text_2, wordvectors)\n",
    "    return similarity(vec_1, vec_2)\n",
    "\n",
    "\n",
    "def similarity(vec_1, vec_2):\n",
    "    sim = vec_1 @ vec_2\n",
    "    return sim\n",
    "\n",
    "def similarity_to_cluster(vec_1, cluster, dist=0):\n",
    "    sims = []\n",
    "    for vec in cluster:\n",
    "        sim = similarity(vec_1, vec)\n",
    "        sims.append(sim)\n",
    "    if dist == 0:\n",
    "        return np.max(sims)\n",
    "    elif dist == 1:\n",
    "        return np.average(sims)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def most_similar(vec_1, vec_set):\n",
    "    best_sim = 0\n",
    "    out = -1\n",
    "    for i,vec in enumerate(vec_set):\n",
    "        sim = similarity(vec_1, vec)\n",
    "        if sim > best_sim:\n",
    "            out = i\n",
    "            best_sim = sim\n",
    "    return out, best_sim\n",
    "    \n",
    "def emb_classify(vec1, emb_train_data, dist=0):\n",
    "    best_sim = 0\n",
    "    predicted_class = -1\n",
    "    for c, cluster in enumerate(emb_train_data):\n",
    "        sim = similarity_to_cluster(vec1, cluster, dist)\n",
    "        if sim > best_sim:\n",
    "            predicted_class = c  \n",
    "            best_sim = sim\n",
    "    return predicted_class, best_sim\n",
    "\n",
    "def save_out_classes(text_cls, text_sims, outfilename):\n",
    "    no_class = ['NA','']\n",
    "    \n",
    "    with open(outfilename, 'w') as outfile:\n",
    "        fieldnames = ['id_movid', 'fecha']\n",
    "        fieldnames += ['con_'+str(i) for i in range(1, len(classes))]\n",
    "        fieldnames += ['class_id', 'text','class', 'sim']\n",
    "        \n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for k in text_cls:\n",
    "            row = {}\n",
    "            i, date = k\n",
    "            row['id_movid'] = i\n",
    "            row['fecha'] = date\n",
    "            row['text'] = text_dict[k]\n",
    "            \n",
    "            for j in range(1, len(classes)):\n",
    "                row['con_'+str(j)] = 0\n",
    "            \n",
    "            if row['text'] in no_class:\n",
    "                class_id = 0\n",
    "                row['sim'] = 0\n",
    "            else:\n",
    "                class_id = labels_to_classes[text_cls[k]]\n",
    "                row['sim'] = text_sims[k]\n",
    "                row['con_'+str(class_id)] = 1\n",
    "                \n",
    "            row['class_id'] = class_id\n",
    "            row['class'] = classes[class_id]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por la fiebre sí, pero estoy con pielonefritis aguda / Sintomas habituales o atribuye a otra causa\n",
      "Por que siempre tengo tos soy asmática / Sintomas habituales o atribuye a otra causa\n",
      "Temor a las personas imprudente, sin ninguncuidado / Considera que no tiene riesgo\n",
      "Tengo colon irritable / Sintomas habituales o atribuye a otra causa\n",
      "No quiero asistir al consultorio por algo sin importancia / Miedo a contagiarse\n",
      "Por que asocio el dolor de cabeza a mi perido premenstrual y la diarrea por algo muy pesado que comi / Sintomas habituales o atribuye a otra causa\n",
      "porque soy jaquecosa, es normal para mí tener dolores de cabeza / Sintomas habituales o atribuye a otra causa\n",
      "tomo medicamento para ello / Sintomas leves o transitorios\n",
      "Porque estoy diagnosticada con fibromialgia y con tratamiento de hace años / Sintomas leves o transitorios\n",
      "no tengo previsión y no puedo salir a un consultorio / Miedo a contagiarse\n",
      "Son síntomas asociados a mi patología miastenia Gravis / Sintomas habituales o atribuye a otra causa\n",
      "Por que lo atribuyo a consumo de cigarrillo, estrés y ejecución de rutinas de ejercicios / Sintomas habituales o atribuye a otra causa\n",
      "Sufro de colon irritable / Sintomas habituales o atribuye a otra causa\n",
      "mi mama es profesional de saldud / Sintomas habituales o atribuye a otra causa\n",
      "Porque fue un dolor de cabeza de un par de días y ya no lo tengo / Sintomas leves o transitorios\n",
      "Es alergia / Sintomas habituales o atribuye a otra causa\n",
      "Porque mis dolores de cabeza son controlados y los he tenido siempre / Sintomas habituales o atribuye a otra causa\n",
      "Porque el síntoma fue sólo un día y lo controlé, con paracetamol. / Sintomas leves o transitorios\n",
      "Porque en condiciones normales, algunas veces me da tos. / Sintomas habituales o atribuye a otra causa\n",
      "No atiende el cesfam / Miedo a contagiarse\n",
      "Por que me pasa a veces.. creo que no soy tolerante a la lactosa / Sintomas habituales o atribuye a otra causa\n",
      "Y porque me evalué el 06/04 y me hicieron el examen que salió negativo / Examen previo negativo/positivo\n",
      "Se me pasó / Considera que no tiene riesgo\n"
     ]
    }
   ],
   "source": [
    "text_cls = {}\n",
    "text_sims = {}\n",
    "for k in emb_dict:\n",
    "    c, sim = emb_classify(emb_dict[k], emb_train_data, dist=0)\n",
    "    text_cls[k] = c\n",
    "    text_sims[k] = sim\n",
    "    if np.random.rand() > 0.998:\n",
    "        print(text_dict[k], '/', classes[labels_to_classes[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outfilename = f'{output}_{today}_classes.csv'\n",
    "save_out_classes(text_cls, text_sims, outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        fieldnames = ['id_movid', 'fecha']\n",
    "        fieldnames += ['con_'+str(i) for i in range(1, len(classes))]\n",
    "        fieldnames += ['class_id', 'text','class', 'sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_movid',\n",
       " 'fecha',\n",
       " 'con_1',\n",
       " 'con_2',\n",
       " 'con_3',\n",
       " 'con_4',\n",
       " 'con_5',\n",
       " 'con_6',\n",
       " 'class_id',\n",
       " 'text',\n",
       " 'class',\n",
       " 'sim']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
